{"openapi":"3.0.3","components":{"schemas":{"PushModelResponse":{"properties":{"total":{"type":"integer","description":"total size of the model","example":"2142590208"},"status":{"$ref":"#/components/schemas/PushModelStatus"},"digest":{"example":"sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711a","description":"the model's digest","type":"string"}},"description":"Response class for pushing a model.","type":"object"},"DeleteModelRequest":{"description":"Request class for deleting a model.","required":["name"],"type":"object","properties":{"name":{"type":"string","example":"llama2:13b","description":"The model name.\n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"}}},"ModelInfo":{"description":"Details about a model including modelfile, template, parameters, license, and system prompt.","type":"object","properties":{"license":{"description":"The model's license.","example":"<contents of license block>","type":"string"},"template":{"example":"[INST] {{ if and .First .System }}<<SYS>>{{ .System }}<</SYS>>\\n\\n{{ end }}{{ .Prompt }} [/INST]","description":"The prompt template for the model.","type":"string"},"parameters":{"type":"string","description":"The model parameters.","example":"stop [INST]\\nstop [/INST]\\nstop <<SYS>>\\nstop <</SYS>>"},"modelfile":{"description":"The modelfile associated with the model.","example":"Modelfile generated by \\\"ollama show\\\"\\n# To build a new Modelfile based on this one, replace the FROM line with:\\n# FROM llama2:latest\\n\\nFROM /Users/username/.ollama/models/blobs/sha256:8daa9615cce30c259a9555b1cc250d461d1bc69980a274b44d7eda0be78076d8\\nTEMPLATE \\\"\\\"\\\"[INST] {{ if and .First .System }}<<SYS>>{{ .System }}<</SYS>>\\n\\n{{ end }}{{ .Prompt }} [/INST] \\\"\\\"\\\"\\nSYSTEM \\\"\\\"\\\"\\\"\\\"\\\"\\nPARAMETER stop [INST]\\nPARAMETER stop [/INST]\\nPARAMETER stop <<SYS>>\\nPARAMETER stop <</SYS>>\\n\"","type":"string"}}},"CreateModelResponse":{"properties":{"status":{"$ref":"#/components/schemas/CreateModelStatus"}},"description":"Response object for creating a model. When finished, `status` is `success`.","type":"object"},"GenerateChatCompletionRequest":{"type":"object","required":["model","messages"],"description":"Request class for the chat endpoint.","properties":{"format":{"$ref":"#/components/schemas/ResponseFormat"},"keep_alive":{"type":"integer","description":"How long (in minutes) to keep the model loaded in memory.\n\n- If set to a positive duration (e.g. 20), the model will stay loaded for the provided duration.\n- If set to a negative duration (e.g. -1), the model will stay loaded indefinitely.\n- If set to 0, the model will be unloaded immediately once finished.\n- If not set, the model will stay loaded for 5 minutes by default\n"},"messages":{"description":"The messages of the chat, this can be used to keep a chat memory","items":{"$ref":"#/components/schemas/Message"},"type":"array"},"stream":{"default":"false","type":"boolean","description":"If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.\n"},"options":{"$ref":"#/components/schemas/RequestOptions"},"model":{"type":"string","example":"llama2:7b","description":"The model name.\n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"}}},"Message":{"type":"object","required":["role","content"],"description":"A message in the chat endpoint","properties":{"images":{"items":{"description":"Base64-encoded image (for multimodal models such as llava)","example":"iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC","type":"string"},"description":"(optional) a list of Base64-encoded images to include in the message (for multimodal models such as llava)","type":"array"},"role":{"enum":["system","user","assistant"],"description":"The role of the message","type":"string"},"content":{"example":"Why is the sky blue?","description":"The content of the message","type":"string"}}},"GenerateCompletionResponse":{"properties":{"created_at":{"format":"date-time","example":"2023-08-04T19:22:45.499127Z","description":"Date on which a model was created.","type":"string"},"prompt_eval_duration":{"description":"Time spent in nanoseconds evaluating the prompt.","example":"1160282000","type":"integer"},"load_duration":{"type":"integer","example":"3013701500","description":"Time spent in nanoseconds loading the model."},"prompt_eval_count":{"description":"Number of tokens in the prompt.","example":"46","type":"integer"},"total_duration":{"type":"integer","example":"5589157167","description":"Time spent generating the response."},"eval_count":{"example":"113","description":"Number of tokens the response.","type":"integer"},"context":{"items":{"type":"integer"},"example":["1","2","3"],"description":"An encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory.\n","type":"array"},"done":{"type":"boolean","description":"Whether the response has completed.","example":"true"},"model":{"description":"The model name.\n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n","example":"llama2:7b","type":"string"},"response":{"description":"The response for a given prompt with a provided model.","example":"The sky appears blue because of a phenomenon called Rayleigh scattering.","type":"string"},"eval_duration":{"description":"Time in nanoseconds spent generating the response.","example":"1325948000","type":"integer"}},"description":"The response class for the generate endpoint.","type":"object"},"PushModelRequest":{"properties":{"stream":{"type":"boolean","default":"false","description":"If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.\n"},"name":{"description":"The name of the model to push in the form of <namespace>/<model>:<tag>.","example":"mattw/pygmalion:latest","type":"string"},"insecure":{"description":"Allow insecure connections to the library.\n\nOnly use this if you are pushing to your library during development.\n","default":"false","type":"boolean"}},"required":["name"],"description":"Request class for pushing a model.","type":"object"},"ModelsResponse":{"type":"object","description":"Response class for the list models endpoint.","properties":{"models":{"type":"array","description":"List of models available locally.","items":{"$ref":"#/components/schemas/Model"}}}},"PullModelStatus":{"type":"string","description":"Status pulling the model.","example":"pulling manifest","enum":["pulling manifest","downloading digestname","verifying sha256 digest","writing manifest","removing any unused layers","success"]},"GenerateEmbeddingResponse":{"properties":{"embedding":{"description":"The embedding for the prompt.","example":["0.5670403838157654","0.009260174818336964","..."],"items":{"format":"double","type":"number"},"type":"array"}},"description":"Returns the embedding information.","type":"object"},"ResponseFormat":{"enum":["json"],"description":"The format to return a response in. Currently the only accepted value is json.\n\nEnable JSON mode by setting the format parameter to json. This will structure the response as valid JSON.\n\nNote: it's important to instruct the model to use JSON in the prompt. Otherwise, the model may generate large amounts whitespace.\n","type":"string"},"PullModelResponse":{"properties":{"digest":{"type":"string","example":"sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711a","description":"The model's digest."},"status":{"$ref":"#/components/schemas/PullModelStatus"},"total":{"example":"2142590208","description":"Total size of the model.","type":"integer"},"completed":{"type":"integer","example":"2142590208","description":"Total bytes transferred."}},"type":"object","description":"Response class for pulling a model.\n\nThe first object is the manifest. Then there is a series of downloading responses. Until any of the download is completed, the `completed` key may not be included.\n\nThe number of files to be downloaded depends on the number of layers specified in the manifest.\n"},"CopyModelRequest":{"properties":{"source":{"example":"llama2:7b","description":"Name of the model to copy.","type":"string"},"destination":{"example":"llama2-backup","description":"Name of the new model.","type":"string"}},"type":"object","description":"Request class for copying a model.","required":["source","destination"]},"ModelInfoRequest":{"properties":{"name":{"type":"string","example":"llama2:7b","description":"The model name.\n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"}},"description":"Request class for the show model info endpoint.","required":["name"],"type":"object"},"Model":{"description":"A model available locally.","type":"object","properties":{"name":{"description":"The model name.\n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n","example":"llama2:7b","type":"string"},"size":{"type":"integer","example":"7323310500","description":"Size of the model on disk."},"modified_at":{"format":"date-time","example":"2023-08-02T17:02:23.713454393-07:00","description":"Model modification date.","type":"string"}}},"GenerateEmbeddingRequest":{"properties":{"model":{"example":"llama2:7b","description":"The model name.\n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n","type":"string"},"options":{"$ref":"#/components/schemas/RequestOptions"},"prompt":{"type":"string","example":"Here is an article about llamas...","description":"Text to generate embeddings for."}},"description":"Generate embeddings from a model.","required":["model","prompt"],"type":"object"},"CreateModelStatus":{"enum":["creating system layer","parsing modelfile","success"],"type":"string","description":"Status creating the model"},"GenerateChatCompletionResponse":{"properties":{"eval_duration":{"example":"1325948000","description":"Time in nanoseconds spent generating the response.","type":"integer"},"model":{"type":"string","description":"The model name.\n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n","example":"llama2:7b"},"done":{"type":"boolean","example":"true","description":"Whether the response has completed."},"eval_count":{"type":"integer","description":"Number of tokens the response.","example":"113"},"prompt_eval_count":{"description":"Number of tokens in the prompt.","example":"46","type":"integer"},"total_duration":{"description":"Time spent generating the response.","example":"5589157167","type":"integer"},"message":{"$ref":"#/components/schemas/Message"},"load_duration":{"type":"integer","example":"3013701500","description":"Time spent in nanoseconds loading the model."},"prompt_eval_duration":{"type":"integer","description":"Time spent in nanoseconds evaluating the prompt.","example":"1160282000"},"created_at":{"format":"date-time","description":"Date on which a model was created.","example":"2023-08-04T19:22:45.499127Z","type":"string"}},"type":"object","description":"The response class for the chat endpoint."},"CreateModelRequest":{"properties":{"name":{"description":"The model name.\n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n","example":"mario","type":"string"},"stream":{"type":"boolean","default":"false","description":"If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.\n"},"modelfile":{"example":"FROM llama2\\nSYSTEM You are mario from Super Mario Bros.","description":"The contents of the Modelfile.","type":"string"}},"type":"object","required":["name","modelfile"],"description":"Create model request object."},"RequestOptions":{"properties":{"use_mmap":{"type":"boolean","description":"Enable mmap. (Default: false)\n"},"mirostat_tau":{"type":"number","description":"Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0)\n","format":"float"},"top_k":{"description":"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\n","type":"integer"},"num_batch":{"description":"Sets the number of batches to use for generation. (Default: 1)\n","type":"integer"},"num_thread":{"type":"integer","description":"Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores).\n"},"mirostat_eta":{"type":"number","description":"Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1)\n","format":"float"},"rope_frequency_scale":{"format":"float","description":"The scale of the rope frequency. (Default: 1.0)\n","type":"number"},"num_keep":{"description":"Number of tokens to keep from the prompt.\n","type":"integer"},"repeat_penalty":{"format":"float","type":"number","description":"Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)\n"},"numa":{"type":"boolean","description":"Enable NUMA support. (Default: false)\n"},"stop":{"description":"Sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.","items":{"type":"string"},"type":"array"},"frequency_penalty":{"format":"float","description":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n","type":"number"},"typical_p":{"format":"float","type":"number","description":"Typical p is used to reduce the impact of less probable tokens from the output.\n"},"presence_penalty":{"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n","type":"number","format":"float"},"f16_kv":{"description":"Enable f16 key/value. (Default: false)\n","type":"boolean"},"tfs_z":{"format":"float","description":"Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (default: 1)\n","type":"number"},"main_gpu":{"type":"integer","description":"The GPU to use for the main model. Default is 0.\n"},"temperature":{"description":"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\n","type":"number","format":"float"},"num_gqa":{"description":"The number of GQA groups in the transformer layer. Required for some models, for example it is 8 for `llama2:70b`.\n","type":"integer"},"vocab_only":{"type":"boolean","description":"Enable vocab only. (Default: false)\n"},"num_predict":{"description":"Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context)\n","type":"integer"},"mirostat":{"type":"integer","description":"Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n"},"logits_all":{"description":"Enable logits all. (Default: false)\n","type":"boolean"},"repeat_last_n":{"type":"integer","description":"Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\n"},"penalize_newline":{"description":"Penalize newlines in the output. (Default: false)\n","type":"boolean"},"rope_frequency_base":{"format":"float","type":"number","description":"The base of the rope frequency scale. (Default: 1.0)\n"},"embedding_only":{"type":"boolean","description":"Enable embedding only. (Default: false)\n"},"num_ctx":{"description":"Sets the size of the context window used to generate the next token.\n","type":"integer"},"low_vram":{"type":"boolean","description":"Enable low VRAM mode. (Default: false)\n"},"use_mlock":{"description":"Enable mlock. (Default: false)\n","type":"boolean"},"num_gpu":{"description":"The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable.\n","type":"integer"},"top_p":{"format":"float","description":"Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\n","type":"number"},"seed":{"type":"integer","description":"Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)\n"}},"description":"Additional model parameters listed in the documentation for the Modelfile such as `temperature`.","type":"object"},"PullModelRequest":{"type":"object","required":["name"],"description":"Request class for pulling a model.","properties":{"stream":{"type":"boolean","default":"false","description":"If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.\n"},"name":{"example":"llama2:7b","description":"The model name.\n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n","type":"string"},"insecure":{"description":"Allow insecure connections to the library.\n\nOnly use this if you are pulling from your own library during development.\n","default":"false","type":"boolean"}}},"GenerateCompletionRequest":{"properties":{"keep_alive":{"type":"integer","description":"How long (in minutes) to keep the model loaded in memory.\n\n- If set to a positive duration (e.g. 20), the model will stay loaded for the provided duration.\n- If set to a negative duration (e.g. -1), the model will stay loaded indefinitely.\n- If set to 0, the model will be unloaded immediately once finished.\n- If not set, the model will stay loaded for 5 minutes by default\n"},"template":{"description":"The full prompt or prompt template (overrides what is defined in the Modelfile).","type":"string"},"raw":{"type":"boolean","description":"If `true` no formatting will be applied to the prompt and no context will be returned.\n\nYou may choose to use the `raw` parameter if you are specifying a full templated prompt in your request to the API, and are managing history yourself.\n"},"prompt":{"type":"string","description":"The prompt to generate a response.","example":"Why is the sky blue?"},"format":{"$ref":"#/components/schemas/ResponseFormat"},"system":{"description":"The system prompt to (overrides what is defined in the Modelfile).","type":"string"},"images":{"items":{"description":"Base64-encoded image (for multimodal models such as llava)","example":"iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC","type":"string"},"description":"(optional) a list of Base64-encoded images to include in the message (for multimodal models such as llava)","type":"array"},"stream":{"description":"If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.\n","default":"false","type":"boolean"},"options":{"$ref":"#/components/schemas/RequestOptions"},"context":{"type":"array","description":"The context parameter returned from a previous request to [generateCompletion], this can be used to keep a short conversational memory.","items":{"type":"integer"}},"model":{"description":"The model name.\n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n","example":"llama2:7b","type":"string"}},"type":"object","required":["model","prompt"],"description":"Request class for the generate endpoint."},"PushModelStatus":{"description":"Status pushing the model.","type":"string","enum":["retrieving manifest","starting upload","pushing manifest","success"]}}},"tags":[{"name":"Completions","description":"Given a prompt, the model will generate a completion."},{"description":"Given a list of messages comprising a conversation, the model will return a response.","name":"Chat"},{"description":"Get a vector representation of a given input.","name":"Embeddings"},{"description":"List and describe the various models available.","name":"Models"}],"servers":[{"url":"http://localhost:11434/api","description":"Ollama server URL"}],"paths":{"/chat":{"post":{"operationId":"generateChatCompletion","requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/GenerateChatCompletionRequest"}}}},"description":"This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.","responses":{"200":{"description":"Successful operation.","content":{"application/x-ndjson":{"schema":{"$ref":"#/components/schemas/GenerateChatCompletionResponse"}}}}},"tags":["Chat"],"summary":"Generate the next message in a chat with a provided model."}},"/create":{"post":{"summary":"Create a model from a Modelfile.","tags":["Models"],"responses":{"200":{"content":{"application/x-ndjson":{"schema":{"$ref":"#/components/schemas/CreateModelResponse"}}},"description":"Successful operation."}},"description":"It is recommended to set `modelfile` to the content of the Modelfile rather than just set `path`. This is a requirement for remote create. Remote model creation should also create any file blobs, fields such as `FROM` and `ADAPTER`, explicitly with the server using Create a Blob and the value to the path indicated in the response.","requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/CreateModelRequest"}}},"description":"Create a new model from a Modelfile."},"operationId":"createModel"}},"/tags":{"get":{"operationId":"listModels","summary":"List models that are available locally.","tags":["Models"],"responses":{"200":{"description":"Successful operation.","content":{"application/json":{"schema":{"$ref":"#/components/schemas/ModelsResponse"}}}}}}},"/embeddings":{"post":{"summary":"Generate embeddings from a model.","tags":["Embeddings"],"responses":{"200":{"description":"Successful operation.","content":{"application/json":{"schema":{"$ref":"#/components/schemas/GenerateEmbeddingResponse"}}}}},"operationId":"generateEmbedding","requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/GenerateEmbeddingRequest"}}}}}},"/delete":{"delete":{"requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/DeleteModelRequest"}}}},"operationId":"deleteModel","tags":["Models"],"summary":"Delete a model and its data.","responses":{"200":{"description":"Successful operation."}}}},"/copy":{"post":{"operationId":"copyModel","requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/CopyModelRequest"}}}},"responses":{"200":{"description":"Successful operation."}},"summary":"Creates a model with another name from an existing model.","tags":["Models"]}},"/push":{"post":{"operationId":"pushModel","requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/PushModelRequest"}}}},"description":"Requires registering for ollama.ai and adding a public key first.","responses":{"200":{"description":"Successful operation.","content":{"application/json":{"schema":{"$ref":"#/components/schemas/PushModelResponse"}}}}},"summary":"Upload a model to a model library.","tags":["Models"]}},"/pull":{"post":{"requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/PullModelRequest"}}}},"operationId":"pullModel","tags":["Models"],"summary":"Download a model from the ollama library.","responses":{"200":{"description":"Successful operation.","content":{"application/json":{"schema":{"$ref":"#/components/schemas/PullModelResponse"}}}}},"description":"Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress."}},"/generate":{"post":{"responses":{"200":{"content":{"application/x-ndjson":{"schema":{"$ref":"#/components/schemas/GenerateCompletionResponse"}}},"description":"Successful operation."}},"tags":["Completions"],"summary":"Generate a response for a given prompt with a provided model.","description":"The final response object will include statistics and additional data from the request.","requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/GenerateCompletionRequest"}}}},"operationId":"generateCompletion"}},"/show":{"post":{"responses":{"200":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/ModelInfo"}}},"description":"Successful operation."}},"summary":"Show details about a model including modelfile, template, parameters, license, and system prompt.","tags":["Models"],"requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/ModelInfoRequest"}}}},"operationId":"showModelInfo"}},"/blobs/{digest}":{"post":{"requestBody":{"content":{"application/octet-stream":{"schema":{"format":"binary","type":"string"}}}},"parameters":[{"required":"true","name":"name","schema":{"type":"string"},"example":"sha256:c8edda1f17edd2f1b60253b773d837bda7b9d249a61245931a4d7c9a8d350250","description":"the SHA256 digest of the blob","in":"query"}],"operationId":"createBlob","tags":["Models"],"summary":"Create a blob from a file. Returns the server file path.","responses":{"201":{"description":"Blob was successfully created"}}},"head":{"summary":"Check to see if a blob exists on the Ollama server which is useful when creating models.","tags":["Models"],"responses":{"404":{"description":"Blob was not found"},"200":{"description":"Blob exists on the server"}},"parameters":[{"name":"name","required":"true","in":"query","description":"the SHA256 digest of the blob","example":"sha256:c8edda1f17edd2f1b60253b773d837bda7b9d249a61245931a4d7c9a8d350250","schema":{"type":"string"}}],"operationId":"checkBlob"}}},"info":{"version":"0.1.9","description":"API Spec for Ollama API. Please see https://github.com/jmorganca/ollama/blob/main/docs/api.md for more details.","title":"Ollama API"}}
