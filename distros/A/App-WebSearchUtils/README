NAME
    App::WebSearchUtils - CLI utilites related to searching with search
    engines

VERSION
    This document describes version 0.001 of App::WebSearchUtils (from Perl
    distribution App-WebSearchUtils), released on 2022-10-10.

SYNOPSIS
    This distribution provides the following utilities:

    *   web-search

FUNCTIONS
  web_search
    Usage:

     web_search(%args) -> [$status_code, $reason, $payload, \%result_meta]

    Open web search page in browser.

    This utility can save you time when you want to open multiple queries
    (with added common prefix/suffix words) or specify some options like
    time limit. It will formulate the search URL(s) then open them for you
    in browser. You can also specify to print out the URLs instead.

    Aside from standard web search, you can also generate/open other
    searches like image, video, news, or map.

    This function is not exported.

    Arguments ('*' denotes required arguments):

    *   action => *str* (default: "open_url")

        What to do with the URLs.

        Instead of opening the queries in browser ("open_url"), you can also
        do other action instead.

        Printing search URLs: "print_url" will print the search URL.
        "print_html_link" will print the HTML link (the <a> tag). And
        "print_org_link" will print the Org-mode link, e.g.
        "[[url...][query]]".

        Saving search result HTMLs: "save_html" will first visit each search
        URL (currently using Firefox::Marionette) then save each result page
        to a file named "<num>-<query>.html" in the current directory.
        Existing files will not be overwritten; the utility will save to
        "*.html.1", "*.html.2" and so on instead.

        Extracting search result links: "print_result_link" will first will
        first visit each search URL (currently using Firefox::Marionette)
        then extract result links and print them. "print_result_html_link"
        and "print_result_org_link" are similar but will instead format each
        link as HTML and Org link, respectively.

        The "print_result_*link" actions are not very useful for some search
        engines like Google because result HTML page is obfuscated. Thus we
        can only extract all links in each page instead of selecting (via
        DOM) only the actual search result entry links, etc.

        If you want to filter the links further by domain, path, etc. you
        can use grep-url.

    *   append => *str*

        String to add at the end of each query.

    *   delay => *duration*

        Delay between opening each query.

        As an alternative to the "--delay" option, you can also use
        "--min-delay" and "--max-delay" to set a random delay between a
        minimum and maximum value.

    *   engine => *str* (default: "google")

        Search engine to use.

    *   max_delay => *duration*

        Delay between opening each query.

    *   min_delay => *duration*

        Delay between opening each query.

        As an alternative to the "--mindelay" and "--max-delay" options, you
        can also use "--delay" to set a constant delay between requests.

    *   num => *posint*

        Number of results per page.

    *   prepend => *str*

        String to add at the beginning of each query.

    *   queries => *array[str]*

    *   queries_from => *filename*

        Supply queries from lines of text file (specify "-" for stdin).

    *   time_end => *date*

    *   time_past => *str*

        Limit time period to the past hour/24hour/week/month/year.

    *   time_start => *date*

    Returns an enveloped result (an array).

    First element ($status_code) is an integer containing HTTP-like status
    code (200 means OK, 4xx caller error, 5xx function error). Second
    element ($reason) is a string containing error message, or something
    like "OK" if status is 200. Third element ($payload) is the actual
    result, but usually not present when enveloped result is an error
    response ($status_code is not 2xx). Fourth element (%result_meta) is
    called result metadata and is optional, a hash that contains extra
    information, much like how HTTP response headers provide additional
    metadata.

    Return value: (any)

HOMEPAGE
    Please visit the project's homepage at
    <https://metacpan.org/release/App-WebSearchUtils>.

SOURCE
    Source repository is at
    <https://github.com/perlancar/perl-App-WebSearchUtils>.

SEE ALSO
    App::GoogleSearchUtils - previous incarnation.

    App::FirefoxMultiAccountContainersUtils.

AUTHOR
    perlancar <perlancar@cpan.org>

CONTRIBUTING
    To contribute, you can send patches by email/via RT, or send pull
    requests on GitHub.

    Most of the time, you don't need to build the distribution yourself. You
    can simply modify the code, then test via:

     % prove -l

    If you want to build the distribution (e.g. to try to install it locally
    on your system), you can install Dist::Zilla,
    Dist::Zilla::PluginBundle::Author::PERLANCAR,
    Pod::Weaver::PluginBundle::Author::PERLANCAR, and sometimes one or two
    other Dist::Zilla- and/or Pod::Weaver plugins. Any additional steps
    required beyond that are considered a bug and can be reported to me.

COPYRIGHT AND LICENSE
    This software is copyright (c) 2022 by perlancar <perlancar@cpan.org>.

    This is free software; you can redistribute it and/or modify it under
    the same terms as the Perl 5 programming language system itself.

BUGS
    Please report any bugs or feature requests on the bugtracker website
    <https://rt.cpan.org/Public/Dist/Display.html?Name=App-WebSearchUtils>

    When submitting a bug or request, please include a test-file or a patch
    to an existing test-file that illustrates the bug or desired feature.

